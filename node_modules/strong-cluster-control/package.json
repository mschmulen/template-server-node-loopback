{
  "name": "strong-cluster-control",
  "version": "0.1.0-1",
  "description": "run-time managment of a node cluster",
  "license": "MIT",
  "main": "index.js",
  "bin": {
    "clusterctl": "bin/cli.js"
  },
  "scripts": {
    "coverage": "./node_modules/.bin/mocha -r blanket -R html-cov > coverage_strong-cluster-control.html",
    "test": "mocha --reporter spec",
    "lint": "./node_modules/.bin/jshint *.js test lib"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/strongloop/strong-cluster-control.git"
  },
  "author": {
    "name": "Sam Roberts",
    "email": "sam@strongloop.com"
  },
  "dependencies": {
    "commander": "~1.3.2",
    "debuglog": "~0.0.2",
    "rc": "~0.3.1"
  },
  "devDependencies": {
    "mocha": "~1.9.0",
    "jshint": "~2.0.1",
    "blanket": "latest"
  },
  "engines": {
    "node": "*"
  },
  "supported": true,
  "readme": "# run-time management of a node cluster\n\n[strong-cluster-control](https://github.com/strongloop/strong-cluster-control)\nis a module for run-time management of a node cluster.\n\nIt is an extension of the node cluster module, not a replacement, and works\nbeside it to add the following features:\n\n- runs `size` workers (optionally), and monitors them for unexpected death\n- run-time control of cluster through command line and API\n- soft shutdown as well as hard termination of workers\n\nIt can be added to an existing application using the node cluster module without\nmodifying how that application is currently starting up or using cluster, and\nstill make use of additional features.\n\nThe controller module allows the cluster to be controlled through the\n\n- clusterctl command line, or\n- API calls on the module\n\n\n## Install\n\n    slnode install strong-cluster-control\n\nThe command line:\n\n    slnode install -g strong-cluster-control\n    clusterctl --help\n\n\n## Example\n\nTo instantiate cluster-control:\n\n```javascript\nvar cluster = require('cluster');\nvar control = require('strong-cluster-control');\n\n// global setup here...\n\ncontrol.start({\n    size: control.CPUS\n}).on('error', function(er) {\n    console.error(er);\n});\n\nif(cluster.isWorker) {\n    // do work here...\n}\n```\n\nTo control the cluster, assuming `my-server` is running in `/apps/`:\n\n    clusterctl --path /apps/my-server/clusterctl set-size 4\n    clusterctl --path /apps/my-server/clusterctl status\n    worker count: 4\n    worker id 0: { pid: 11454 }\n    worker id 1: { pid: 11471 }\n    worker id 2: { pid: 11473 }\n    worker id 3: { pid: 11475 }\n\nFor more in-depth examples, see the [chat server example](https://github.com/strongloop/slnode-examples/tree/master/chat),\nand the\n[in-source example](https://github.com/strongloop/strong-cluster-control/blob/master/bin/example-master.js).\n\n\n## clusterctl: command line interface\n\nThe `clusterctl` command line utility can be used to control a cluster at\nrun-time. It defaults to communicating over the `clusterctl` named socket\nin the current working directory, but an explicit path or port can be\nprovided.\n\nIt provides the following commands:\n\n- status: reports the status of the cluster workers\n- set-size: set cluster size to some number of workers\n- disconnect: disconnect all workers\n- fork: fork one worker\n\n`disconnect` and `fork` cause the cluster size to change, so new workers will\nprobably be started or stopped to return the cluster to the set size. They are\nprimarily for testing and development.\n\n\n## Controller API\n\n### control = require('strong-cluster-control')\n\nThe controller is exported by the `strong-cluster-control` module.\n\n### control.start([options],[callback])\n\nStart the controller.\n\n* `options`: {Object} An options object, see below for supported properties,\n  no default, and options object is not required.\n* `callback`: {Function} A callback function, it is set as a listener for\n  the `'start'` event.\n\nThe options are:\n\n* `size`: {Integer} Number of workers that should be running, the default\n  is to *not* control the number of workers, see `setSize()`\n\n* `env`: {Object} Environment properties object passed to `cluster.fork()` if\n  the controller has to start a worker to resize the cluster, default is null.\n\n* `addr`: {String or Integer} Address to listen on for control, defaults to first of\n  `options.path`, `options.port`, or `control.ADDR` that is defined.\n* `path`: {String} Path to listen on for control, no default.\n* `port`: {Integer} Localhost port to listen on for control, may be necessary to\n  use on Windows, no default.\n\n* `shutdownTimeout`: {Milliseconds} Number of milliseconds to wait after\n  shutdown before terminating a worker, the default is 5 seconds, see\n  `.shutdown()`\n* `terminateTimeout`: {Milliseconds} Number of milliseconds to wait after\n  terminate before killing a worker, the default is 5 seconds, see\n  `.terminate()`\n\nFor convenience during setup, it is not necessary to wrap `.start()` in a protective\nconditional `if(cluster.isMaster) {control.start()}`, when called in workers it quietly\ndoes nothing but call its callback.\n\nThe 'start' event is emitted after the controller is started.\n\n### control.stop([callback])\n\nStop the controller. Remove event listeners that were set on the `cluster`\nmodule, and stop listening on the control port.\n\n* `callback`: {Function} A callback function, it is set as a listener for\n  the `'stop'` event.\n\nThe 'stop' event is emitted after the controller is stopped.\n\n### control.loadOptions([defaults])\n\nLoad options from configuration files, environment, or command line.\n\n* `defaults`: {Object} Default options, see `start()` for description\n  of supported options.\n\nAn options object is returned that is suitable for passing directly to\n`start()`. How you use it is up to you, but it can be conveniently used to\nimplement optionally clustered applications, ones that run unclustered when\ndeployed as single instances, perhaps behind a load balancer, or that can be\ndeployed as a node cluster.\n\nHere is an example of the above usage pattern:\n\n    // app.js\n    var control = require('strong-cluster-control');\n    var options = control.loadOptions();\n    if(options.clustered && options.isMaster) {\n        return control.start(options);\n    }\n\n    // Server setup... or any work that should be done in the master if\n    // the application is not being clustered, or in the worker if it\n    // is being clustered.\n\nThe options values are derived from the following configuration sources:\n\n- command line arguments (parsed by optimist): if your app ignores unknown\n  options, options can be set on the command line, ex. `node app.js --size=2`\n- environment variables prefixed with `cluster_`: note that the variable must be\n  lower case, ex. `cluster_size=2 node app.js`\n- `.clusterrc` in either json or ini format: can be in the current working\n  directory, or any parent paths\n- `defaults`: as passed in, if any\n\nThis is actually a subset of the possible locations, the\n[rc](https://npmjs.org/package/rc) module is used with an `appname` of\n\"cluster\", see it's documentation for more information.\n\nSupported values are those described in `start()`, with a few extensions for\n`size`, which may be one of:\n\n1. a positive integer, or a string that converts to a positive integer, as for `start()`\n2. `\"default\"`, or a string containing `\"cpu\"`, this will be converted to the\n   number of cpus, see `control.CPUS`\n3. `0`, `\"off\"`, or anything else that isn't one of the previous values will be\n   converted to `0`, and indicate a preference for *not* clustering\n\nThe returned options object will contain the following fields that are not\noptions to `start()`:\n\n- clustered: false if size is 0 (after above conversions), true if a cluster\n  size was specified\n- isMaster, isWorker: identical to the properties of the same name in the\n  cluster module\n\nThe combination of the above three allows you to determine if you are a worker,\nor not, and if you are a master, if you should start the cluster control module,\nor just start the server if clustering was not requested.\n\n\n### control.setSize(N)\n\nSet the size of the cluster.\n\n* `N`: {Integer or null} The size of the cluster is the number of workers\n  that should be maintained online. A size of `null` clears the size, and\n  disables the size control feature of the controller.\n\nThe cluster size can be set explicitly with `setSize()`, or implicitly through\nthe `options` provided to `.start()`, or through the control channel.\n\nThe size cannot be set until the controller has been started, and will not be\nmaintained after the cluster has stopped.\n\nOnce set, the controller will listen on cluster `fork` and `exit` events,\nand resize the cluster back to the set size if necessary. After the cluster has\nbeen resized, the 'resize' event will be emitted.\n\nWhen a resize is necessary, workers will be started or stopped one-by-one until\nthe cluster is the set size.\n\nCluster workers are started with `cluster.fork(control.options.env)`, so the\nenvironment can be set, but must be the same for each worker. After a worker has\nbeen started, the 'startWorker' event will be emitted.\n\nCluster workers are stopped with `.shutdown()`. After a worker has been stopped,\nthe 'stopWorker' event will be emitted.\n\n### control.shutdown(id)\n\nDisconnect worker `id` and take increasingly agressive action until it exits.\n\n* `id` {Number} Cluster worker ID, see `cluster.workers` in [cluster docs](http://nodejs.org/docs/latest/api/cluster.html)\n\nThe effect of disconnect on a worker is to close all the servers in the worker,\nwait for them to close, and then exit. This process may not occur in a timely\nfashion if, for example, the server connections do not close. In order to\ngracefully close any open connections, a worker may listen to the `SHUTDOWN`\nmessage, see `control.cmd.SHUTDOWN`.\n\nSends a `SHUTDOWN` message to the identified worker, calls\n`worker.disconnect()`, and sets a timer for `control.options.shutdownTimeout`.\nIf the worker has not exited by that time, calls `.terminate()` on the worker.\n\n### control.terminate(id)\n\nTerminate worker `id`, taking increasingly aggressive action until it exits.\n\n* `id` {Number} Cluster worker ID, see `cluster.workers` in [cluster docs](http://nodejs.org/docs/latest/api/cluster.html)\n\nThe effect of sending SIGTERM to a node process should be to cause it to exit.\nThis may not occur in a timely fashion if, for example, the process is ignoring\nSIGTERM, or busy looping.\n\nCalls `worker.kill(\"SIGTERM\")` on the identified worker, and sets a timer for\n`control.options.terminateTimeout`. If the worker has not exited by that time,\ncalls `worker.(\"SIGKILL\")` on the worker.\n\n### control.options\n\nThe options set by calling `.start()`.\n\nVisible for diagnostic and logging purposes, do *not* modify the options\ndirectly.\n\n### control.cmd.SHUTDOWN\n\n* {String} `'CLUSTER_CONTROL_shutdown'`\n\nThe `SHUTDOWN` message is sent by `.shutdown()` before disconnecting the worker,\nand can be used to gracefully close any open connections before the\n`control.options.shutdownTimeout` expires.\n\nAll connections will be closed at the TCP level when the worker exits or is\nterminated, but this message gives the opportunity to close at a more\napplication-appropriate time, for example, after any outstanding requests have\nbeen completed.\n\nThe message format is:\n\n    { cmd: control.cmd.SHUTDOWN }\n\nIt can be received in a worker by listening for a `'message'` event with a\nmatching `cmd` property:\n\n    process.on('message', function(msg) {\n        if(msg.cmd === control.cmd.SHUTDOWN) {\n            // Close any open connections as soon as they are idle...\n        }\n    });\n\n\n### control.ADDR\n\n`'clusterctl'`, the default address of the control server, if none other are\nspecified through `options`.\n\n### control.CPUS\n\nThe number of CPUs reported by node's `os.cpus().length`, this is a good default\nfor the cluster size, in the absence of application specific analysis of what\nwould be an optimal number of workers.\n\n\n### Event: 'start'\n\nEmitted after control has started. Currently, control is considered started\nafter the 'listening' event has been emitted.\n\nStarting of workers happens in the background, if you are specifically\ninterested in knowning when all the workers have started, see the 'resize'\nevent.\n\n### Event: 'error'\n\n* {Error Object}\n\nEmitted when an error occurs. The only current source of errors is the control\nprotocol, which may require logging of the errors, but should not effect the\noperation of the controller.\n\n### Event: 'resize'\n\n* {Integer} size, the number of workers now that resize is complete (will always\n  be the same as `cluster.options.size`)\n\nEmitted after a resize of the cluster is complete. At this point, no more\nworkers will be forked or shutdown by the controller until either the size is\nchanged or workers fork or exit, see `setSize()`.\n\n### Event: 'startWorker'\n\n* `worker` {Worker object}\n\nEmitted after a worker which was started during a resize comes online, see the\nnode API documentation for description of `worker` and \"online\".\n\n### Event: 'stopWorker'\n\n* `worker` {Worker object}\n* `code` {Number} the exit code, if it exited normally.\n* `signal` {String} the name of the signal if it exited due to signal\n\nEmitted after a worker which was shutdown during a resize exits, see the node\nAPI documentation for a description of `worker`.\n\nThe values of `code` and `signal`, as well as of `worker.suicide`, can be used\nto determine how gracefully the worker was stopped. See `.terminate()`.\n\n### Event: 'listening'\n\nEmitted when the controller has been bound after calling `server.listen`.\n\n### Event: 'connection'\n\n* {Socket object} The connection object\n\nEmitted when a new connection is made. `socket` is an instance of\n`net.Socket`.\n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/strongloop/strong-cluster-control/issues"
  },
  "_id": "strong-cluster-control@0.1.0-1",
  "_from": "strong-cluster-control@~0.1.0"
}
